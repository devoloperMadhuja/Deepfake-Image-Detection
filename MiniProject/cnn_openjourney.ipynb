{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_32jHGE2qfIM",
        "outputId": "ca131709-0bd2-418d-eeb3-c9ab3c634806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Models will be saved in: /content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "NEW_MODEL_DIR = '/content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW'\n",
        "\n",
        "os.makedirs(NEW_MODEL_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Models will be saved in: {NEW_MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Unzip BOTH Datasets\n",
        "import os\n",
        "\n",
        "print(\"--- Step 1: Unzipping Datasets ---\")\n",
        "\n",
        "\n",
        "DRIVE_ZIP_PATH_REAL = '/content/drive/My Drive/DeepFakeDataset/140k-real-and-fake-faces.zip'\n",
        "LOCAL_DATA_PATH_REAL = '/content/dataset_140k'\n",
        "\n",
        "\n",
        "DRIVE_ZIP_PATH_FAKE = '/content/drive/My Drive/DeepFakeDataset/openjourney.zip'\n",
        "LOCAL_DATA_PATH_FAKE = '/content/dataset_openjourney'\n",
        "\n",
        "\n",
        "if not os.path.exists(os.path.join(LOCAL_DATA_PATH_REAL, 'real_vs_fake')):\n",
        "    print(\"Unzipping 140k Real dataset...\")\n",
        "    !rm -rf \"{LOCAL_DATA_PATH_REAL}\"\n",
        "    os.makedirs(LOCAL_DATA_PATH_REAL, exist_ok=True)\n",
        "    !unzip -q \"{DRIVE_ZIP_PATH_REAL}\" -d \"{LOCAL_DATA_PATH_REAL}\"\n",
        "else:\n",
        "    print(\"140k Real dataset already unzipped.\")\n",
        "\n",
        "if not os.path.exists(os.path.join(LOCAL_DATA_PATH_FAKE, 'openjourney')):\n",
        "    print(\"Unzipping openjourney FAKE dataset...\")\n",
        "    !rm -rf \"{LOCAL_DATA_PATH_FAKE}\"\n",
        "    os.makedirs(LOCAL_DATA_PATH_FAKE, exist_ok=True)\n",
        "    !unzip -q \"{DRIVE_ZIP_PATH_FAKE}\" -d \"{LOCAL_DATA_PATH_FAKE}\"\n",
        "else:\n",
        "    print(\"openjourney FAKE dataset already unzipped.\")\n",
        "\n",
        "print(\"‚úÖ All data ready for training.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUIhuW0TsEs8",
        "outputId": "a9629be2-d6d8-4663-fa6d-ea64954794e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Unzipping Datasets ---\n",
            "Unzipping 140k Real dataset...\n",
            "Unzipping openjourney FAKE dataset...\n",
            "‚úÖ All data ready for training.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"--- Verifying top 50 lines of FAKE dataset ---\")\n",
        "!ls -lR '/content/dataset_openjourney' | head -n 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObyTqAWQsE1v",
        "outputId": "b0d2fe1a-d98a-4a32-9aa8-263e2bb0c1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying top 50 lines of FAKE dataset ---\n",
            "/content/dataset_openjourney:\n",
            "total 568\n",
            "drwxrwxrwx 2 root root 577536 Nov  1 16:07 openjourney\n",
            "\n",
            "/content/dataset_openjourney/openjourney:\n",
            "total 5740076\n",
            "-rw-rw-rw- 1 root root 399534 Nov  1 16:04 image_10_0_0.png\n",
            "-rw-rw-rw- 1 root root 367156 Nov  1 16:04 image_10_0_100.png\n",
            "-rw-rw-rw- 1 root root 437729 Nov  1 16:04 image_10_0_101.png\n",
            "-rw-rw-rw- 1 root root 401984 Nov  1 16:04 image_10_0_102.png\n",
            "-rw-rw-rw- 1 root root 461694 Nov  1 16:04 image_10_0_103.png\n",
            "-rw-rw-rw- 1 root root 415023 Nov  1 16:04 image_10_0_104.png\n",
            "-rw-rw-rw- 1 root root 391276 Nov  1 16:04 image_10_0_105.png\n",
            "-rw-rw-rw- 1 root root 442336 Nov  1 16:04 image_10_0_106.png\n",
            "-rw-rw-rw- 1 root root 346818 Nov  1 16:04 image_10_0_107.png\n",
            "-rw-rw-rw- 1 root root 305512 Nov  1 16:04 image_10_0_108.png\n",
            "-rw-rw-rw- 1 root root 544811 Nov  1 16:04 image_10_0_109.png\n",
            "-rw-rw-rw- 1 root root 435849 Nov  1 16:04 image_10_0_10.png\n",
            "-rw-rw-rw- 1 root root 436681 Nov  1 16:04 image_10_0_110.png\n",
            "-rw-rw-rw- 1 root root 414604 Nov  1 16:04 image_10_0_111.png\n",
            "-rw-rw-rw- 1 root root 397267 Nov  1 16:04 image_10_0_112.png\n",
            "-rw-rw-rw- 1 root root 491932 Nov  1 16:04 image_10_0_113.png\n",
            "-rw-rw-rw- 1 root root 442194 Nov  1 16:04 image_10_0_114.png\n",
            "-rw-rw-rw- 1 root root 380976 Nov  1 16:04 image_10_0_115.png\n",
            "-rw-rw-rw- 1 root root 419286 Nov  1 16:04 image_10_0_116.png\n",
            "-rw-rw-rw- 1 root root 478452 Nov  1 16:04 image_10_0_117.png\n",
            "-rw-rw-rw- 1 root root 371846 Nov  1 16:04 image_10_0_118.png\n",
            "-rw-rw-rw- 1 root root 472482 Nov  1 16:04 image_10_0_119.png\n",
            "-rw-rw-rw- 1 root root 377115 Nov  1 16:04 image_10_0_11.png\n",
            "-rw-rw-rw- 1 root root 388011 Nov  1 16:04 image_10_0_120.png\n",
            "-rw-rw-rw- 1 root root 465383 Nov  1 16:04 image_10_0_121.png\n",
            "-rw-rw-rw- 1 root root 454596 Nov  1 16:04 image_10_0_122.png\n",
            "-rw-rw-rw- 1 root root 417839 Nov  1 16:04 image_10_0_123.png\n",
            "-rw-rw-rw- 1 root root 376778 Nov  1 16:04 image_10_0_124.png\n",
            "-rw-rw-rw- 1 root root 486869 Nov  1 16:04 image_10_0_125.png\n",
            "-rw-rw-rw- 1 root root 397915 Nov  1 16:04 image_10_0_126.png\n",
            "-rw-rw-rw- 1 root root 440884 Nov  1 16:04 image_10_0_127.png\n",
            "-rw-rw-rw- 1 root root 338361 Nov  1 16:04 image_10_0_128.png\n",
            "-rw-rw-rw- 1 root root 468447 Nov  1 16:04 image_10_0_129.png\n",
            "-rw-rw-rw- 1 root root 415708 Nov  1 16:04 image_10_0_12.png\n",
            "-rw-rw-rw- 1 root root 349509 Nov  1 16:04 image_10_0_130.png\n",
            "-rw-rw-rw- 1 root root 497403 Nov  1 16:04 image_10_0_131.png\n",
            "-rw-rw-rw- 1 root root 419493 Nov  1 16:04 image_10_0_132.png\n",
            "-rw-rw-rw- 1 root root 401998 Nov  1 16:04 image_10_0_133.png\n",
            "-rw-rw-rw- 1 root root 223707 Nov  1 16:04 image_10_0_134.png\n",
            "-rw-rw-rw- 1 root root 345212 Nov  1 16:04 image_10_0_135.png\n",
            "-rw-rw-rw- 1 root root 432795 Nov  1 16:04 image_10_0_136.png\n",
            "-rw-rw-rw- 1 root root 339955 Nov  1 16:04 image_10_0_137.png\n",
            "-rw-rw-rw- 1 root root 398065 Nov  1 16:04 image_10_0_138.png\n",
            "-rw-rw-rw- 1 root root 434867 Nov  1 16:04 image_10_0_139.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define the ImageDataset Class\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_list, transform=None):\n",
        "        self.data_list = data_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data_list[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "\n",
        "            return torch.zeros((3, 224, 224)), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "print(\"ImageDataset class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmShbd3xsE_w",
        "outputId": "8fff459a-e204-460d-f7a6-f79eab54f601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageDataset class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define the SimpleCNN Model Architecture (with Regularization)\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(32 * 56 * 56, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "print(\"SimpleCNN model class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n5Cvd7QsFG2",
        "outputId": "34b67170-8412-449f-8ec9-e940d67e81c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN model class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create the Data Loaders\n",
        "import glob\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\n--- Step 2: Preparing Data Loaders ---\")\n",
        "\n",
        "\n",
        "REAL_DATA_PATH = '/content/dataset_140k/real_vs_fake/real-vs-fake'\n",
        "real_files = glob.glob(os.path.join(REAL_DATA_PATH, 'train/real', '*.jpg')) + \\\n",
        "             glob.glob(os.path.join(REAL_DATA_PATH, 'valid/real', '*.jpg'))\n",
        "print(f\"Found {len(real_files)} total REAL images.\")\n",
        "\n",
        "\n",
        "FAKE_DATA_PATH = '/content/dataset_openjourney/openjourney'\n",
        "fake_files = glob.glob(os.path.join(FAKE_DATA_PATH, '*.png'))\n",
        "print(f\"Found {len(fake_files)} total FAKE (openjourney) images.\")\n",
        "\n",
        "\n",
        "if len(fake_files) == 0:\n",
        "    print(\"‚ùå ERROR: No fake files found. Cannot continue.\")\n",
        "    print(f\"Please check the path: {FAKE_DATA_PATH}\")\n",
        "else:\n",
        "    real_files_balanced = random.sample(real_files, len(fake_files))\n",
        "    print(f\"Balancing dataset: Using {len(real_files_balanced)} REAL images.\")\n",
        "\n",
        "\n",
        "    all_files_list = [(path, 0) for path in fake_files] + \\\n",
        "                     [(path, 1) for path in real_files_balanced]\n",
        "\n",
        "    labels = [label for path, label in all_files_list]\n",
        "    train_list, valid_list = train_test_split(\n",
        "        all_files_list,\n",
        "        test_size=0.20,\n",
        "        random_state=42,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Found {len(all_files_list)} total balanced images.\")\n",
        "    print(f\"Split into {len(train_list)} training images.\")\n",
        "    print(f\"Split into {len(valid_list)} validation images.\")\n",
        "\n",
        "\n",
        "    im_size = 224\n",
        "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((im_size, im_size)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "\n",
        "    valid_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((im_size, im_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    train_data = ImageDataset(train_list, transform=train_transforms)\n",
        "    valid_data = ImageDataset(valid_list, transform=valid_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"‚úÖ Data loaders are ready.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF__vd3IsFNu",
        "outputId": "003aa731-70f5-4c5e-a8d0-2681bbf490bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 2: Preparing Data Loaders ---\n",
            "Found 60000 total REAL images.\n",
            "Found 14204 total FAKE (openjourney) images.\n",
            "Balancing dataset: Using 14204 REAL images.\n",
            "Found 28408 total balanced images.\n",
            "Split into 22726 training images.\n",
            "Split into 5682 validation images.\n",
            "‚úÖ Data loaders are ready.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Train the Model (CHANGED: New Path, Weight Decay, Early Stopping)\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "print(\"--- Step 3: Setting Up for Training ---\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "MODEL_DIR = '/content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW'\n",
        "\n",
        "\n",
        "MODEL_NAME = 'openjourney-simple-cnn-v3-full'\n",
        "\n",
        "BEST_MODEL_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}_best_model.pth')\n",
        "CHECKPOINT_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}_checkpoint.pth')\n",
        "\n",
        "print(f\"Models and checkpoints will be saved in: {MODEL_DIR}\")\n",
        "\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "lr = 1e-4\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "patience = 4\n",
        "epochs_no_improve = 0\n",
        "best_valid_acc = 0.0\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_valid_acc = checkpoint.get('best_valid_acc', 0.0)\n",
        "    epochs_no_improve = checkpoint.get('epochs_no_improve', 0)\n",
        "    print(f\"‚úÖ Checkpoint found. Resuming training from epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "print(f\"Training for {num_epochs} total epochs on device: {device}\\n\")\n",
        "\n",
        "\n",
        "total_train_time = 0.0\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    valid_loss = running_loss / len(valid_loader.dataset)\n",
        "    valid_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    total_train_time += epoch_time\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} ({epoch_time:.2f}s) | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} Acc: {valid_acc:.4f}\")\n",
        "\n",
        "\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"üéâ New best model saved with accuracy: {best_valid_acc:.4f}\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Validation accuracy did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
        "\n",
        "\n",
        "    checkpoint_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_valid_acc': best_valid_acc,\n",
        "        'epochs_no_improve': epochs_no_improve\n",
        "    }\n",
        "    torch.save(checkpoint_data, CHECKPOINT_PATH)\n",
        "    print(f\"üíæ Checkpoint saved for epoch {epoch+1}.\\n\")\n",
        "\n",
        "    # --- NEW: Check for early stopping ---\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"--- üõë Early stopping triggered after {patience} epochs with no improvement. ---\")\n",
        "        break\n",
        "\n",
        "\n",
        "print(f\"--- TRAINING COMPLETE ---\")\n",
        "print(f\"Total training time: {total_train_time:.2f} seconds\")\n",
        "print(f\"Best model saved to: {BEST_MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywNdT8OqsFR0",
        "outputId": "5535824a-a29b-40c1-9f20-a7aef641ddc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 3: Setting Up for Training ---\n",
            "Models and checkpoints will be saved in: /content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW\n",
            "‚ÑπÔ∏è No checkpoint found. Starting training from scratch.\n",
            "Training for 10 total epochs on device: cuda\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:31<00:00,  3.36it/s]\n",
            "Epoch 1/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:49<00:00,  3.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 (260.95s) | Train Loss: 0.1384 Acc: 0.9544 | Valid Loss: 0.0516 Acc: 0.9808\n",
            "üéâ New best model saved with accuracy: 0.9808\n",
            "üíæ Checkpoint saved for epoch 1.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:24<00:00,  3.47it/s]\n",
            "Epoch 2/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:49<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 (254.14s) | Train Loss: 0.0640 Acc: 0.9782 | Valid Loss: 0.0535 Acc: 0.9836\n",
            "üéâ New best model saved with accuracy: 0.9836\n",
            "üíæ Checkpoint saved for epoch 2.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:28<00:00,  3.41it/s]\n",
            "Epoch 3/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:47<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 (256.14s) | Train Loss: 0.0520 Acc: 0.9828 | Valid Loss: 0.0394 Acc: 0.9866\n",
            "üéâ New best model saved with accuracy: 0.9866\n",
            "üíæ Checkpoint saved for epoch 3.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:27<00:00,  3.42it/s]\n",
            "Epoch 4/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:49<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 (257.49s) | Train Loss: 0.0452 Acc: 0.9846 | Valid Loss: 0.0349 Acc: 0.9887\n",
            "üéâ New best model saved with accuracy: 0.9887\n",
            "üíæ Checkpoint saved for epoch 4.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:24<00:00,  3.48it/s]\n",
            "Epoch 5/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:49<00:00,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 (253.60s) | Train Loss: 0.0365 Acc: 0.9870 | Valid Loss: 0.0280 Acc: 0.9887\n",
            "Validation accuracy did not improve. Patience: 1/4\n",
            "üíæ Checkpoint saved for epoch 5.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:22<00:00,  3.52it/s]\n",
            "Epoch 6/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:49<00:00,  3.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 (251.41s) | Train Loss: 0.0364 Acc: 0.9880 | Valid Loss: 0.0248 Acc: 0.9908\n",
            "üéâ New best model saved with accuracy: 0.9908\n",
            "üíæ Checkpoint saved for epoch 6.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:26<00:00,  3.45it/s]\n",
            "Epoch 7/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:47<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 (253.88s) | Train Loss: 0.0285 Acc: 0.9890 | Valid Loss: 0.0247 Acc: 0.9908\n",
            "Validation accuracy did not improve. Patience: 1/4\n",
            "üíæ Checkpoint saved for epoch 7.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:21<00:00,  3.54it/s]\n",
            "Epoch 8/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:48<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 (249.50s) | Train Loss: 0.0283 Acc: 0.9905 | Valid Loss: 0.0210 Acc: 0.9933\n",
            "üéâ New best model saved with accuracy: 0.9933\n",
            "üíæ Checkpoint saved for epoch 8.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:19<00:00,  3.56it/s]\n",
            "Epoch 9/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:47<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 (247.88s) | Train Loss: 0.0279 Acc: 0.9901 | Valid Loss: 0.0220 Acc: 0.9940\n",
            "üéâ New best model saved with accuracy: 0.9940\n",
            "üíæ Checkpoint saved for epoch 9.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 711/711 [03:19<00:00,  3.56it/s]\n",
            "Epoch 10/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:46<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 (246.32s) | Train Loss: 0.0249 Acc: 0.9918 | Valid Loss: 0.0255 Acc: 0.9912\n",
            "Validation accuracy did not improve. Patience: 1/4\n",
            "üíæ Checkpoint saved for epoch 10.\n",
            "\n",
            "--- TRAINING COMPLETE ---\n",
            "Total training time: 2531.31 seconds\n",
            "Best model saved to: /content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW/openjourney-simple-cnn-v3-full_best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VvKH-YOssFVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVDPYzwTsFZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ENLSkoSMsFdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FvRHWzWsFnC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}