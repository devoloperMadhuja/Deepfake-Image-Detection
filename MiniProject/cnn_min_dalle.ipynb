{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkAxf3Z-__qe",
        "outputId": "bb6f572a-ae10-4574-ad0d-7a218cbcd7aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Models will be saved in: /content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Google Drive & Define Path\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- NEW: Define the new save directory ---\n",
        "# As you confirmed, this path already exists\n",
        "NEW_MODEL_DIR = '/content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW'\n",
        "print(f\"‚úÖ Models will be saved in: {NEW_MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Unzip BOTH Datasets\n",
        "import os\n",
        "\n",
        "print(\"--- Step 1: Unzipping Datasets ---\")\n",
        "\n",
        "# --- Path to your REAL images dataset ---\n",
        "DRIVE_ZIP_PATH_REAL = '/content/drive/My Drive/DeepFakeDataset/140k-real-and-fake-faces.zip'\n",
        "LOCAL_DATA_PATH_REAL = '/content/dataset_140k'\n",
        "\n",
        "# --- Path to your FAKE (min-dalle) images dataset ---\n",
        "DRIVE_ZIP_PATH_FAKE = '/content/drive/My Drive/DeepFakeDataset/min-dalle.zip'\n",
        "LOCAL_DATA_PATH_FAKE = '/content/dataset_min-dalle'\n",
        "\n",
        "# Unzip REAL dataset (if not already done)\n",
        "if not os.path.exists(os.path.join(LOCAL_DATA_PATH_REAL, 'real_vs_fake')):\n",
        "    print(\"Unzipping 140k Real dataset...\")\n",
        "    !rm -rf \"{LOCAL_DATA_PATH_REAL}\"\n",
        "    os.makedirs(LOCAL_DATA_PATH_REAL, exist_ok=True)\n",
        "    !unzip -q \"{DRIVE_ZIP_PATH_REAL}\" -d \"{LOCAL_DATA_PATH_REAL}\"\n",
        "else:\n",
        "    print(\"140k Real dataset already unzipped.\")\n",
        "\n",
        "# Unzip FAKE dataset (if not already done)\n",
        "if not os.path.exists(os.path.join(LOCAL_DATA_PATH_FAKE, 'min-dalle')):\n",
        "    print(\"Unzipping min-dalle FAKE dataset...\")\n",
        "    !rm -rf \"{LOCAL_DATA_PATH_FAKE}\"\n",
        "    os.makedirs(LOCAL_DATA_PATH_FAKE, exist_ok=True)\n",
        "    !unzip -q \"{DRIVE_ZIP_PATH_FAKE}\" -d \"{LOCAL_DATA_PATH_FAKE}\"\n",
        "else:\n",
        "    print(\"min-dalle FAKE dataset already unzipped.\")\n",
        "\n",
        "print(\"‚úÖ All data ready for training.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV1ulY7aAODq",
        "outputId": "06d4f9d4-a7c7-4938-d849-c9241f2e9e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Unzipping Datasets ---\n",
            "Unzipping 140k Real dataset...\n",
            "Unzipping min-dalle FAKE dataset...\n",
            "‚úÖ All data ready for training.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: VERIFY Unzipped Structure\n",
        "print(\"--- Verifying top 50 lines of FAKE dataset ---\")\n",
        "!ls -lR '/content/dataset_min-dalle' | head -n 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vl1bXf8AOLi",
        "outputId": "3f4928a4-d334-4f1e-fd0f-f143ef29df72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying top 50 lines of FAKE dataset ---\n",
            "/content/dataset_min-dalle:\n",
            "total 468\n",
            "drwxrwxrwx 2 root root 475136 Nov  1 16:04 min-dalle\n",
            "\n",
            "/content/dataset_min-dalle/min-dalle:\n",
            "total 1211996\n",
            "-rw-rw-rw- 1 root root  99587 Nov  1 16:02 image_1_0_0.png\n",
            "-rw-rw-rw- 1 root root 108529 Nov  1 16:02 image_1_0_100.png\n",
            "-rw-rw-rw- 1 root root 121898 Nov  1 16:02 image_1_0_101.png\n",
            "-rw-rw-rw- 1 root root  96017 Nov  1 16:02 image_1_0_102.png\n",
            "-rw-rw-rw- 1 root root 103403 Nov  1 16:02 image_1_0_103.png\n",
            "-rw-rw-rw- 1 root root 156462 Nov  1 16:02 image_1_0_104.png\n",
            "-rw-rw-rw- 1 root root 118729 Nov  1 16:02 image_1_0_105.png\n",
            "-rw-rw-rw- 1 root root 130406 Nov  1 16:02 image_1_0_106.png\n",
            "-rw-rw-rw- 1 root root 110302 Nov  1 16:02 image_1_0_107.png\n",
            "-rw-rw-rw- 1 root root 108706 Nov  1 16:02 image_1_0_108.png\n",
            "-rw-rw-rw- 1 root root  90299 Nov  1 16:02 image_1_0_109.png\n",
            "-rw-rw-rw- 1 root root 117343 Nov  1 16:02 image_1_0_10.png\n",
            "-rw-rw-rw- 1 root root 146733 Nov  1 16:02 image_1_0_110.png\n",
            "-rw-rw-rw- 1 root root 112966 Nov  1 16:02 image_1_0_111.png\n",
            "-rw-rw-rw- 1 root root  98652 Nov  1 16:02 image_1_0_112.png\n",
            "-rw-rw-rw- 1 root root  94018 Nov  1 16:02 image_1_0_113.png\n",
            "-rw-rw-rw- 1 root root  98605 Nov  1 16:02 image_1_0_114.png\n",
            "-rw-rw-rw- 1 root root 125515 Nov  1 16:02 image_1_0_115.png\n",
            "-rw-rw-rw- 1 root root 115686 Nov  1 16:02 image_1_0_116.png\n",
            "-rw-rw-rw- 1 root root  99776 Nov  1 16:02 image_1_0_117.png\n",
            "-rw-rw-rw- 1 root root  85023 Nov  1 16:02 image_1_0_118.png\n",
            "-rw-rw-rw- 1 root root 124614 Nov  1 16:02 image_1_0_119.png\n",
            "-rw-rw-rw- 1 root root 115586 Nov  1 16:02 image_1_0_11.png\n",
            "-rw-rw-rw- 1 root root  95962 Nov  1 16:02 image_1_0_120.png\n",
            "-rw-rw-rw- 1 root root 122408 Nov  1 16:02 image_1_0_121.png\n",
            "-rw-rw-rw- 1 root root 132755 Nov  1 16:02 image_1_0_122.png\n",
            "-rw-rw-rw- 1 root root 176275 Nov  1 16:02 image_1_0_123.png\n",
            "-rw-rw-rw- 1 root root 111936 Nov  1 16:02 image_1_0_124.png\n",
            "-rw-rw-rw- 1 root root 102288 Nov  1 16:02 image_1_0_125.png\n",
            "-rw-rw-rw- 1 root root 106021 Nov  1 16:02 image_1_0_126.png\n",
            "-rw-rw-rw- 1 root root 101975 Nov  1 16:02 image_1_0_127.png\n",
            "-rw-rw-rw- 1 root root 111468 Nov  1 16:02 image_1_0_128.png\n",
            "-rw-rw-rw- 1 root root  88996 Nov  1 16:02 image_1_0_129.png\n",
            "-rw-rw-rw- 1 root root 108684 Nov  1 16:02 image_1_0_12.png\n",
            "-rw-rw-rw- 1 root root 101642 Nov  1 16:02 image_1_0_130.png\n",
            "-rw-rw-rw- 1 root root 152919 Nov  1 16:02 image_1_0_131.png\n",
            "-rw-rw-rw- 1 root root  97102 Nov  1 16:02 image_1_0_132.png\n",
            "-rw-rw-rw- 1 root root 110457 Nov  1 16:02 image_1_0_133.png\n",
            "-rw-rw-rw- 1 root root 148331 Nov  1 16:02 image_1_0_134.png\n",
            "-rw-rw-rw- 1 root root 121172 Nov  1 16:02 image_1_0_135.png\n",
            "-rw-rw-rw- 1 root root 101008 Nov  1 16:02 image_1_0_136.png\n",
            "-rw-rw-rw- 1 root root 135225 Nov  1 16:02 image_1_0_137.png\n",
            "-rw-rw-rw- 1 root root 124100 Nov  1 16:02 image_1_0_138.png\n",
            "-rw-rw-rw- 1 root root  99618 Nov  1 16:02 image_1_0_139.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define the ImageDataset Class\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_list, transform=None):\n",
        "        self.data_list = data_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data_list[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Handle images that might fail to load\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "            # Return a blank image and the label\n",
        "            return torch.zeros((3, 224, 224)), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # CrossEntropyLoss requires labels as LongTensor\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "print(\"ImageDataset class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JSqf3uwAOVy",
        "outputId": "23af8e9a-5f4c-4b76-a81d-21ce1c3c1788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageDataset class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define the SimpleCNN Model Architecture (with Regularization)\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # --- CHANGED: Added BatchNorm layers ---\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16), # Helps with generalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32), # Helps with generalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # --- CHANGED: Added Dropout layer ---\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(32 * 56 * 56, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5), # \"Turns off\" 50% of neurons to prevent co-adaptation\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "print(\"SimpleCNN model class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LhWjkYKAOci",
        "outputId": "93c8835e-3583-4843-9e8b-3f123f443b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN model class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create the Data Loaders (Balanced, NO Augmentation - FASTEST)\n",
        "import glob\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\n--- Step 2: Preparing Data Loaders ---\")\n",
        "\n",
        "# --- 1. Load REAL files from 140k dataset ---\n",
        "REAL_DATA_PATH = '/content/dataset_140k/real_vs_fake/real-vs-fake'\n",
        "real_files = glob.glob(os.path.join(REAL_DATA_PATH, 'train/real', '*.jpg')) + \\\n",
        "             glob.glob(os.path.join(REAL_DATA_PATH, 'valid/real', '*.jpg'))\n",
        "print(f\"Found {len(real_files)} total REAL images.\")\n",
        "\n",
        "# --- 2. Load FAKE files (min-dalle ONLY) ---\n",
        "FAKE_DATA_PATH = '/content/dataset_min-dalle/min-dalle'\n",
        "fake_files = glob.glob(os.path.join(FAKE_DATA_PATH, '*.png'))\n",
        "print(f\"Found {len(fake_files)} total FAKE (min-dalle) images.\")\n",
        "\n",
        "# --- 3. Balance the dataset ---\n",
        "if len(fake_files) == 0:\n",
        "    print(\"‚ùå ERROR: No fake files found. Cannot continue.\")\n",
        "    print(f\"Please check the path: {FAKE_DATA_PATH}\")\n",
        "else:\n",
        "    real_files_balanced = random.sample(real_files, len(fake_files))\n",
        "    print(f\"Balancing dataset: Using {len(real_files_balanced)} REAL images.\")\n",
        "\n",
        "    # --- 4. Create master list and split ---\n",
        "    all_files_list = [(path, 0) for path in fake_files] + \\\n",
        "                     [(path, 1) for path in real_files_balanced]\n",
        "\n",
        "    labels = [label for path, label in all_files_list]\n",
        "    train_list, valid_list = train_test_split(\n",
        "        all_files_list,\n",
        "        test_size=0.20,\n",
        "        random_state=42,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Found {len(all_files_list)} total balanced images.\")\n",
        "    print(f\"Split into {len(train_list)} training images.\")\n",
        "    print(f\"Split into {len(valid_list)} validation images.\")\n",
        "\n",
        "    # --- 5. Create Transforms (CHANGED: NO Augmentation) ---\n",
        "    im_size = 224\n",
        "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "    # --- NEW: NO Augmentation for EITHER set for max speed ---\n",
        "    # Both train and validation now use the same simple transform\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((im_size, im_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    train_data = ImageDataset(train_list, transform=data_transforms)\n",
        "    valid_data = ImageDataset(valid_list, transform=data_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"‚úÖ Data loaders are ready.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m9uPn9QAOhh",
        "outputId": "928d6594-2846-4dae-f6c0-66d876edc029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 2: Preparing Data Loaders ---\n",
            "Found 60000 total REAL images.\n",
            "Found 10969 total FAKE (min-dalle) images.\n",
            "Balancing dataset: Using 10969 REAL images.\n",
            "Found 21938 total balanced images.\n",
            "Split into 17550 training images.\n",
            "Split into 4388 validation images.\n",
            "‚úÖ Data loaders are ready.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Train the Model (CHANGED: New Path, Weight Decay, Early Stopping)\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "print(\"--- Step 3: Setting Up for Training ---\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- CHANGED: Using the new path you requested ---\n",
        "MODEL_DIR = '/content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW'\n",
        "\n",
        "# --- CHANGED: Using the v2 name as requested ---\n",
        "MODEL_NAME = 'min-dalle-simple-cnn-v2'\n",
        "\n",
        "BEST_MODEL_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}_best_model.pth')\n",
        "CHECKPOINT_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}_checkpoint.pth')\n",
        "\n",
        "print(f\"Models and checkpoints will be saved in: {MODEL_DIR}\")\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = SimpleCNN().to(device) # This uses the Cell 5 model with Dropout/BatchNorm\n",
        "lr = 1e-4\n",
        "num_epochs = 10\n",
        "\n",
        "# --- CHANGED: Added weight_decay (L2 regularization) ---\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- NEW: Variables for Early Stopping ---\n",
        "patience = 7 # Stop if no improvement after 7 epochs\n",
        "epochs_no_improve = 0\n",
        "best_valid_acc = 0.0\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_valid_acc = checkpoint.get('best_valid_acc', 0.0)\n",
        "    epochs_no_improve = checkpoint.get('epochs_no_improve', 0)\n",
        "    print(f\"‚úÖ Checkpoint found. Resuming training from epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "print(f\"Training for {num_epochs} total epochs on device: {device}\\n\")\n",
        "\n",
        "# --- Training loop ---\n",
        "total_train_time = 0.0\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    valid_loss = running_loss / len(valid_loader.dataset)\n",
        "    valid_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    total_train_time += epoch_time\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} ({epoch_time:.2f}s) | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} Acc: {valid_acc:.4f}\")\n",
        "\n",
        "    # --- CHANGED: Updated saving and early stopping logic ---\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"üéâ New best model saved with accuracy: {best_valid_acc:.4f}\")\n",
        "        epochs_no_improve = 0 # Reset patience\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Validation accuracy did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
        "\n",
        "    # --- Save checkpoint after every epoch ---\n",
        "    checkpoint_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_valid_acc': best_valid_acc,\n",
        "        'epochs_no_improve': epochs_no_improve # Save patience counter\n",
        "    }\n",
        "    torch.save(checkpoint_data, CHECKPOINT_PATH)\n",
        "    print(f\"üíæ Checkpoint saved for epoch {epoch+1}.\\n\")\n",
        "\n",
        "    # --- NEW: Check for early stopping ---\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"--- üõë Early stopping triggered after {patience} epochs with no improvement. ---\")\n",
        "        break\n",
        "\n",
        "\n",
        "print(f\"--- TRAINING COMPLETE ---\")\n",
        "print(f\"Total training time: {total_train_time:.2f} seconds\")\n",
        "print(f\"Best model saved to: {BEST_MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4Q349qyAOlc",
        "outputId": "6c900fd1-f15a-4179-b606-5226afec5f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 3: Setting Up for Training ---\n",
            "Models and checkpoints will be saved in: /content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW\n",
            "‚ÑπÔ∏è No checkpoint found. Starting training from scratch.\n",
            "Training for 10 total epochs on device: cuda\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:14<00:00,  7.36it/s]\n",
            "Epoch 1/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:16<00:00,  8.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 (91.66s) | Train Loss: 0.1032 Acc: 0.9667 | Valid Loss: 0.0422 Acc: 0.9868\n",
            "üéâ New best model saved with accuracy: 0.9868\n",
            "üíæ Checkpoint saved for epoch 1.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:10<00:00,  7.75it/s]\n",
            "Epoch 2/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:17<00:00,  7.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 (88.30s) | Train Loss: 0.0243 Acc: 0.9929 | Valid Loss: 0.0123 Acc: 0.9954\n",
            "üéâ New best model saved with accuracy: 0.9954\n",
            "üíæ Checkpoint saved for epoch 2.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:12<00:00,  7.52it/s]\n",
            "Epoch 3/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:16<00:00,  8.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 (89.15s) | Train Loss: 0.0103 Acc: 0.9966 | Valid Loss: 0.0043 Acc: 0.9995\n",
            "üéâ New best model saved with accuracy: 0.9995\n",
            "üíæ Checkpoint saved for epoch 3.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:12<00:00,  7.57it/s]\n",
            "Epoch 4/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:16<00:00,  8.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 (88.55s) | Train Loss: 0.0124 Acc: 0.9952 | Valid Loss: 0.0073 Acc: 0.9977\n",
            "Validation accuracy did not improve. Patience: 1/7\n",
            "üíæ Checkpoint saved for epoch 4.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:11<00:00,  7.69it/s]\n",
            "Epoch 5/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:17<00:00,  7.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 (88.76s) | Train Loss: 0.0063 Acc: 0.9981 | Valid Loss: 0.0028 Acc: 0.9991\n",
            "Validation accuracy did not improve. Patience: 2/7\n",
            "üíæ Checkpoint saved for epoch 5.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:10<00:00,  7.80it/s]\n",
            "Epoch 6/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:16<00:00,  8.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 (86.44s) | Train Loss: 0.0057 Acc: 0.9983 | Valid Loss: 0.0020 Acc: 0.9995\n",
            "Validation accuracy did not improve. Patience: 3/7\n",
            "üíæ Checkpoint saved for epoch 6.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:11<00:00,  7.65it/s]\n",
            "Epoch 7/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:15<00:00,  8.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 (87.73s) | Train Loss: 0.0081 Acc: 0.9966 | Valid Loss: 0.0015 Acc: 0.9998\n",
            "üéâ New best model saved with accuracy: 0.9998\n",
            "üíæ Checkpoint saved for epoch 7.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:13<00:00,  7.46it/s]\n",
            "Epoch 8/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:15<00:00,  8.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 (89.35s) | Train Loss: 0.0063 Acc: 0.9978 | Valid Loss: 0.0048 Acc: 0.9986\n",
            "Validation accuracy did not improve. Patience: 1/7\n",
            "üíæ Checkpoint saved for epoch 8.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:10<00:00,  7.73it/s]\n",
            "Epoch 9/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:17<00:00,  7.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 (88.36s) | Train Loss: 0.0090 Acc: 0.9968 | Valid Loss: 0.0087 Acc: 0.9973\n",
            "Validation accuracy did not improve. Patience: 2/7\n",
            "üíæ Checkpoint saved for epoch 9.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [01:10<00:00,  7.77it/s]\n",
            "Epoch 10/10 [Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138/138 [00:16<00:00,  8.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 (86.79s) | Train Loss: 0.0027 Acc: 0.9991 | Valid Loss: 0.0046 Acc: 0.9993\n",
            "Validation accuracy did not improve. Patience: 3/7\n",
            "üíæ Checkpoint saved for epoch 10.\n",
            "\n",
            "--- TRAINING COMPLETE ---\n",
            "Total training time: 885.07 seconds\n",
            "Best model saved to: /content/drive/My Drive/DeepFakeDataset/NEW MODELS/NEW/min-dalle-simple-cnn-v2_best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4rTq2n1AOo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ip6doXh3AOsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mEpyhW12AOw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61IuETZMAO2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pt-aMdKIAO7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}